Model Architecture

    Input Size:
        The input to the model consists of a 60-day sequence of stock features: Open, High, Low, Close, and Volume.
        Input dimensions: (batch_size, sequence_length=60, features=5).

Choice of Hyperparameters

    Optimization Process:
        The hyperparameters were optimized using Optuna, a powerful hyperparameter optimization library.
        Objective functions were defined for both LSTM and GRU models to minimize validation loss.

    Selected Hyperparameters:
        LSTM:
            num_layers: 3
            hidden_size: 116
            learning_rate: 0.00406
            batch_size: 16
        GRU:
            num_layers: 2
            hidden_size: 71
            learning_rate: 0.00616
            batch_size: 16

    Explanation of Choices:
        Number of Layers:
            More layers allow the model to learn deeper temporal dependencies.
        Hidden Size:
            Larger hidden sizes enable the model to capture complex patterns but can lead to overfitting. A balance was achieved through optimization.
        Learning Rate:
            A learning rate in the range of 1e-4 to 1e-2 ensures effective gradient descent convergence.
        Batch Size:
            Smaller batch sizes (e.g., 16) balance computational efficiency and gradient noise for better generalization.

Training Enhancements

    Early Stopping:
        Monitors validation loss during training and stops the process when no significant improvement is observed for 3 consecutive epochs.
        Prevents overfitting and reduces training time.

    Loss Function:
        Mean Squared Error (MSE) was chosen to measure the prediction error for the continuous target variable.

    Evaluation Metrics:
        MSE: Measures average squared prediction errors.
		
Improvements

    Training Data Augmentation:
        Sequences were generated using a rolling window over historical data to maximize training samples.
    Normalization:
        MinMaxScaler ensured all features were scaled to [0, 1], allowing models to converge faster and avoid dominance of high-value features.
		
